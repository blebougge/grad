# readItems - no arg
{'and': 'LOG', 'int': 'TYPE', 'ary': 'ARRAY_DCL', 'eq': 'LOG', 'if': 'IF', '1': 'CONST', '#': 'COMMENT', 'end': 'END', '0': 'CONST', ')': 'END_PAR', '(': 'ST_PAR', 'fix': 'CONST_VAR', '*': 'OP', '-': 'OP', ',': 'COMMA', '/': 'OP', 'write': 'FUNC', 'read': 'FUNC', '3': 'CONST', '2': 'CONST', '5': 'CONST', '4': 'CONST', '7': 'CONST', '6': 'CONST', 'prog': 'PROG', '8': 'CONST', ';': 'IDENT', ':': 'DEF', '=': 'EQ_OP', '>': 'LOG', 'begin': 'BEGIN', '!=': 'LOG', 'bl': 'TYPE', '9': 'CONST', 'not': 'LOG', 'dob': 'TYPE', 'or': 'LOG', 'while': 'LOOP', 'str': 'TYPE', '+': 'OP'}
# readItems - correct use
{'and': 'LOG', 'int': 'TYPE', 'ary': 'ARRAY_DCL', 'eq': 'LOG', 'if': 'IF', '1': 'CONST', '#': 'COMMENT', 'end': 'END', '0': 'CONST', ')': 'END_PAR', '(': 'ST_PAR', 'fix': 'CONST_VAR', '*': 'OP', '-': 'OP', ',': 'COMMA', '/': 'OP', 'write': 'FUNC', 'read': 'FUNC', '3': 'CONST', '2': 'CONST', '5': 'CONST', '4': 'CONST', '7': 'CONST', '6': 'CONST', 'prog': 'PROG', '8': 'CONST', ';': 'IDENT', ':': 'DEF', '=': 'EQ_OP', '>': 'LOG', 'begin': 'BEGIN', '!=': 'LOG', 'bl': 'TYPE', '9': 'CONST', 'not': 'LOG', 'dob': 'TYPE', 'or': 'LOG', 'while': 'LOOP', 'str': 'TYPE', '+': 'OP'}

# tokenizer - lang.items lexars
{0: '<prog, PROG>', 1: '<PROG, ???>', 2: '<;, IDENT>', 3: '<begin, BEGIN>', 4: '<BEGIN, ???>', 5: '<;, IDENT>', 6: '<end, END>', 7: '<END, ???>', 8: '<;, IDENT>', 9: '<fix, CONST_VAR>', 10: '<CONST_VAR, ???>', 11: '<;, IDENT>', 12: '<ary, ARRAY_DCL>', 13: '<ARRAY_DCL, ???>', 14: '<;, IDENT>', 15: '<:, DEF>', 16: '<DEF, ???>', 17: '<;, IDENT>', 18: '<(, ST_PAR>', 19: '<ST_PAR, ???>', 20: '<;, IDENT>', 21: '<), END_PAR>', 22: '<END_PAR, ???>', 23: '<;, IDENT>', 24: '<# COMMENT, COMMENT>', 25: '<;, IDENT>', 26: '<,, COMMA>', 27: '<COMMA, ???>', 28: '<;, IDENT>', 29: '<int, TYPE>', 30: '<TYPE, ???>', 31: '<;, IDENT>', 32: '<dob, TYPE>', 33: '<TYPE, ???>', 34: '<;, IDENT>', 35: '<bl, TYPE>', 36: '<TYPE, ???>', 37: '<;, IDENT>', 38: '<str, TYPE>', 39: '<TYPE, ???>', 40: '<;, IDENT>', 41: '<while, LOOP>', 42: '<LOOP, ???>', 43: '<;, IDENT>', 44: '<if, IF>', 45: '<IF, ???>', 46: '<;, IDENT>', 47: '<read, FUNC>', 48: '<FUNC, ???>', 49: '<;, IDENT>', 50: '<write, FUNC>', 51: '<FUNC, ???>', 52: '<;, IDENT>', 53: '<+, OP>', 54: '<OP, ???>', 55: '<;, IDENT>', 56: '<-, OP>', 57: '<OP, ???>', 58: '<;, IDENT>', 59: '<*, OP>', 60: '<OP, ???>', 61: '<;, IDENT>', 62: '</, OP>', 63: '<OP, ???>', 64: '<;, IDENT>', 65: '<=, EQ_OP>', 66: '<EQ_OP, ???>', 67: '<;, IDENT>', 68: '<>, LOG>', 69: '<LOG, ???>', 70: '<;, IDENT>', 71: '<eq, LOG>', 72: '<LOG, ???>', 73: '<;, IDENT>', 74: '<!, ???>', 75: '<=, EQ_OP>', 76: '<LOG, ???>', 77: '<;, IDENT>', 78: '<not, LOG>', 79: '<LOG, ???>', 80: '<;, IDENT>', 81: '<and, LOG>', 82: '<LOG, ???>', 83: '<;, IDENT>', 84: '<or, LOG>', 85: '<LOG, ???>', 86: '<;, IDENT>', 87: '<;, IDENT>', 88: '<IDENT, ???>', 89: '<;, IDENT>', 90: '<1, CONST>', 91: '<CONST, ???>', 92: '<;, IDENT>', 93: '<2, CONST>', 94: '<CONST, ???>', 95: '<;, IDENT>', 96: '<3, CONST>', 97: '<CONST, ???>', 98: '<;, IDENT>', 99: '<4, CONST>', 100: '<CONST, ???>', 101: '<;, IDENT>', 102: '<5, CONST>', 103: '<CONST, ???>', 104: '<;, IDENT>', 105: '<6, CONST>', 106: '<CONST, ???>', 107: '<;, IDENT>', 108: '<7, CONST>', 109: '<CONST, ???>', 110: '<;, IDENT>', 111: '<8, CONST>', 112: '<CONST, ???>', 113: '<;, IDENT>', 114: '<9, CONST>', 115: '<CONST, ???>', 116: '<;, IDENT>', 117: '<0, CONST>', 118: '<CONST, ???>', 119: '<;, IDENT>'}
# tokenizer - langError.in lexars
{0: '<# caian 05/11/2015, COMMENT>', 1: '<;, IDENT>', 2: '<pro, ???>', 3: '<lang.in, ???>', 4: '<;, IDENT>', 5: '<fix, CONST_VAR>', 6: '<int, TYPE>', 7: '<:, DEF>', 8: '<id, ???>', 9: '<=, EQ_OP>', 10: '<1, CONST>', 11: '<;, IDENT>', 12: '<fi, ???>', 13: '<st, ???>', 14: '<:, DEF>', 15: '<.out, ???>', 16: '<=, EQ_OP>', 17: '<"file_name, ???>', 18: '<;, IDENT>', 19: '<begin, BEGIN>', 20: '<;, IDENT>', 21: '<int, TYPE>', 22: '<:, DEF>', 23: '<x, ???>', 24: '<=, EQ_OP>', 25: '<2, CONST>', 26: '<;, IDENT>', 27: '<x, ???>', 28: '<x, ???>', 29: '<+, OP>', 30: '<id, ???>', 31: '<;, IDENT>', 32: '<if, IF>', 33: '<(, ST_PAR>', 34: '<x, ???>', 35: '<2, CONST>', 36: '<), END_PAR>', 37: '<;, IDENT>', 38: '<begin, BEGIN>', 39: '<;, IDENT>', 40: '<wrie, ???>', 41: '<(, ST_PAR>', 42: '<ile_name, ???>', 43: '<,, COMMA>', 44: '<x, ???>', 45: '<), END_PAR>', 46: '<;, IDENT>', 47: '<end, END>', 48: '<;, IDENT>', 49: '<str, TYPE>', 50: '<:, DEF>', 51: '<file, ???>', 52: '<;, IDENT>', 53: '<if, IF>', 54: '<eq, LOG>', 55: '<2, CONST>', 56: '<), END_PAR>', 57: '<;, IDENT>', 58: '<bein, ???>', 59: '<;, IDENT>', 60: '<file, ???>', 61: '<=, EQ_OP>', 62: '<read, FUNC>', 63: '<(, ST_PAR>', 64: '<file_ame, ???>', 65: '<;, IDENT>', 66: '<en, ???>', 67: '<;, IDENT>', 68: '<end, END>', 69: '<;, IDENT>'}
# tokenizer - lang.in lexars
{0: '<# caian 05/11/2015, COMMENT>', 1: '<;, IDENT>', 2: '<prog, PROG>', 3: '<lang.in, ???>', 4: '<;, IDENT>', 5: '<fix, CONST_VAR>', 6: '<int, TYPE>', 7: '<:, DEF>', 8: '<id, ???>', 9: '<=, EQ_OP>', 10: '<1, CONST>', 11: '<;, IDENT>', 12: '<fix, CONST_VAR>', 13: '<str, TYPE>', 14: '<:, DEF>', 15: '<f.out, ???>', 16: '<=, EQ_OP>', 17: '<"file_name", ???>', 18: '<;, IDENT>', 19: '<begin, BEGIN>', 20: '<;, IDENT>', 21: '<int, TYPE>', 22: '<:, DEF>', 23: '<x, ???>', 24: '<=, EQ_OP>', 25: '<2, CONST>', 26: '<;, IDENT>', 27: '<x, ???>', 28: '<=, EQ_OP>', 29: '<x, ???>', 30: '<+, OP>', 31: '<id, ???>', 32: '<;, IDENT>', 33: '<if, IF>', 34: '<(, ST_PAR>', 35: '<x, ???>', 36: '<>, LOG>', 37: '<2, CONST>', 38: '<), END_PAR>', 39: '<;, IDENT>', 40: '<begin, BEGIN>', 41: '<;, IDENT>', 42: '<write, FUNC>', 43: '<(, ST_PAR>', 44: '<file_name, ???>', 45: '<,, COMMA>', 46: '<x, ???>', 47: '<), END_PAR>', 48: '<;, IDENT>', 49: '<end, END>', 50: '<;, IDENT>', 51: '<str, TYPE>', 52: '<:, DEF>', 53: '<file, ???>', 54: '<;, IDENT>', 55: '<if, IF>', 56: '<(, ST_PAR>', 57: '<x, ???>', 58: '<eq, LOG>', 59: '<20, CONST>', 60: '<), END_PAR>', 61: '<;, IDENT>', 62: '<begin, BEGIN>', 63: '<;, IDENT>', 64: '<file, ???>', 65: '<=, EQ_OP>', 66: '<read, FUNC>', 67: '<(, ST_PAR>', 68: '<file_name, ???>', 69: '<), END_PAR>', 70: '<;, IDENT>', 71: '<end, END>', 72: '<;, IDENT>', 73: '<end, END>', 74: '<;, IDENT>'}
# tokenizer - NEW TOKENIZER lang.in lexars
None
